# -*- coding: utf-8 -*-
"""
í†µí•© LLM í˜¸ì¶œ ìœ í‹¸
- OpenAI: GPT-4o, GPT-4o-mini
- Google: Gemini 2.0 Flash
- OpenRouter: Llama-3.1-70B (OpenAI í˜¸í™˜)
- Groq: Llama-3.1-70B (OpenAI í˜¸í™˜)
- Ollama: ë¡œì»¬ llama3
ì‚¬ìš©ë²•:
  1) ì•„ë˜ LLM_Model ê°’ì„ ì›í•˜ëŠ” í”„ë¡œë°”ì´ë”ë¡œ ì„¤ì •
  2) call_llm(prompt) í˜¸ì¶œ (str ë˜ëŠ” [SystemMessage, HumanMessage] ë¦¬ìŠ¤íŠ¸)
"""

import os
import time
from typing import List, Union

from langchain_google_genai import ChatGoogleGenerativeAI
from openai import OpenAI, OpenAIError
from langchain_core.messages import SystemMessage, HumanMessage

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ë¡œê¹… ìœ í‹¸ (ì§ˆë¬¸ì—ì„œ ì£¼ì‹  ê·¸ëŒ€ë¡œ ì‚¬ìš©)
from common.token_survey import add_openai

# ====== ì„¤ì • ======
# "openai" | "gemini" | "ollama" | "openrouter_llama" | "groq_llama"
LLM_Model = "openai"

# ê¸°ë³¸ ëª¨ë¸ëª…(í•„ìš” ì‹œ ë³€ê²½ ê°€ëŠ¥)
OPENAI_MODEL = "gpt-4o"            # ë˜ëŠ” "gpt-4o" or "gpt-4o-mini"
GEMINI_MODEL = "gemini-2.0-flash"  # "gemini-2.0-flash" ,"gemini-2.5-flash"
OPENROUTER_LLAMA_MODEL = "meta-llama/llama-3.1-70b-instruct"
GROQ_LLAMA_MODEL = "llama-3.3-70b-versatile"  # Groq ëª¨ë¸ëª…


def call_llm(prompt: Union[str, List[HumanMessage]]) -> str:
    print(f"ğŸ” Using LLM model: {LLM_Model}")
    if LLM_Model == "openai":
        return call_openai(prompt)
    elif LLM_Model == "gemini":
        return call_gemini(prompt)
    elif LLM_Model == "ollama":
        return call_ollama(prompt)
    elif LLM_Model == "openrouter_llama":
        return call_openrouter_llama(prompt)
    elif LLM_Model == "groq_llama":
        return call_groq_llama(prompt)
    else:
        raise ValueError(f"âŒ Unknown LLM_MODEL: {LLM_Model}")


# ====== ë‚´ë¶€ í—¬í¼ ======
def _build_messages(prompt: Union[str, List[HumanMessage]]):
    """OpenAI í˜¸í™˜ messages êµ¬ì„±"""
    if isinstance(prompt, list):
        msgs = []
        for msg in prompt:
            if isinstance(msg, SystemMessage):
                msgs.append({"role": "system", "content": msg.content})
            elif isinstance(msg, HumanMessage):
                msgs.append({"role": "user", "content": msg.content})
            else:
                raise ValueError(f"âŒ Unknown message type: {type(msg)}")
        return msgs
    elif isinstance(prompt, str):
        return [{"role": "user", "content": prompt}]
    else:
        raise ValueError("âŒ prompt must be str or list of messages")


# ====== OpenAI (GPT-4o / 4o-mini) ======
def call_openai(prompt: Union[str, List[HumanMessage]]) -> str:
    """
    OPENAI_API_KEY í•„ìš”
    """
    try:
        client = OpenAI()  # OPENAI_API_KEY ì½ìŒ
        start_time = time.time()
        messages = _build_messages(prompt)

        if OPENAI_MODEL == "gpt-5" or OPENAI_MODEL == "gpt-5-mini":
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                max_completion_tokens=16000,
            )
        else:
            response = client.chat.completions.create(
                model=OPENAI_MODEL,          # "gpt-4o" ë˜ëŠ” "gpt-4o-mini"
                messages=messages,
                temperature=0.0,   
                    
                # max_tokens=16000,
            )

        elapsed = time.time() - start_time
        # content = (response.choices[0].message.content or "").strip().lower()
        content = (response.choices[0].message.content or "").strip()

        usage = getattr(response, "usage", None) or {}
        prompt_tokens = getattr(usage, "prompt_tokens", 0) or 0
        completion_tokens = getattr(usage, "completion_tokens", 0) or 0
        total_tokens = getattr(usage, "total_tokens", 0) or (prompt_tokens + completion_tokens)
        add_openai(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens)

        print(f"âœ… OpenAI ì‘ë‹µ ì™„ë£Œ: {elapsed:.2f}s | Prompt: {prompt_tokens} | Completion: {completion_tokens} | Total: {total_tokens}")
        return content or "analyze"

    except OpenAIError as e:
        print(f"âŒ OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "analyze"
    except Exception as e:
        print(f"âŒ OpenAI ì˜ˆì™¸: {e}")
        return "analyze"


# ====== OpenRouter (Llama 70B) ======
def call_openrouter_llama(prompt: Union[str, List[HumanMessage]]) -> str:
    """
    OPENROUTER_API_KEY í•„ìš”
    - OpenAI SDK í˜¸í™˜: base_urlë§Œ OpenRouterë¡œ êµì²´
    """
    try:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise RuntimeError("OPENROUTER_API_KEY is not set")

        client = OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        start_time = time.time()
        messages = _build_messages(prompt)

        response = client.chat.completions.create(
            model=OPENROUTER_LLAMA_MODEL,  # "meta-llama/llama-3.1-70b-instruct"
            messages=messages,
            temperature=0.0,
            max_tokens=16384,
            # í•„ìš” ì‹œ JSON ê°•ì œ(ì§€ì› í˜¸ìŠ¤íŒ…ì—ì„œë§Œ):
            # response_format={"type": "json_object"},
        )

        elapsed = time.time() - start_time
        content = (response.choices[0].message.content or "").strip().lower()

        usage = getattr(response, "usage", None) or {}
        prompt_tokens = getattr(usage, "prompt_tokens", 0) or 0
        completion_tokens = getattr(usage, "completion_tokens", 0) or 0
        total_tokens = getattr(usage, "total_tokens", 0) or (prompt_tokens + completion_tokens)
        add_openai(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens)

        print(f"âœ… OpenRouter(Llama-70B) ì™„ë£Œ: {elapsed:.2f}s | Prompt: {prompt_tokens} | Completion: {completion_tokens} | Total: {total_tokens}")
        return content or "analyze"

    except Exception as e:
        print(f"âŒ OpenRouter Llama í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "analyze"


# ====== Groq (Llama 70B) ======
def call_groq_llama(prompt: Union[str, List[HumanMessage]]) -> str:
    """
    GROQ_API_KEY í•„ìš”
    - OpenAI SDK í˜¸í™˜: base_urlë§Œ Groqë¡œ êµì²´
    """
    try:
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise RuntimeError("GROQ_API_KEY is not set")

        client = OpenAI(
            api_key=api_key,
            base_url="https://api.groq.com/openai/v1"
        )
        start_time = time.time()
        messages = _build_messages(prompt)

        response = client.chat.completions.create(
            model=GROQ_LLAMA_MODEL,  # "llama3-70b-8192" (8Bë„ ê°€ëŠ¥: "llama3-8b-8192")
            messages=messages,
            temperature=0.0,
            max_tokens=16384,
        )

        elapsed = time.time() - start_time
        content = (response.choices[0].message.content or "").strip().lower()

        usage = getattr(response, "usage", None) or {}
        prompt_tokens = getattr(usage, "prompt_tokens", 0) or 0
        completion_tokens = getattr(usage, "completion_tokens", 0) or 0
        total_tokens = getattr(usage, "total_tokens", 0) or (prompt_tokens + completion_tokens)
        add_openai(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens)

        print(f"âœ… Groq(Llama-70B) ì‘ë‹µ ì™„ë£Œ: {elapsed:.2f}s | Prompt: {prompt_tokens} | Completion: {completion_tokens} | Total: {total_tokens}")
        return content or "analyze"

    except Exception as e:
        print(f"âŒ Groq í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "analyze"


# ====== Gemini 2.0 Flash ======
def call_gemini(prompt: Union[str, List[HumanMessage]]) -> str:
    """
    GOOGLE_API_KEY í•„ìš”
    - langchain_google_genai ê°€ ë‚´ë¶€ì—ì„œ GOOGLE_API_KEYë¥¼ ì‚¬ìš©
    """
    try:
        llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL, temperature=0.0)
        start_time = time.time()

        # langchain ì¸í„°í˜ì´ìŠ¤ëŠ” messages ë¦¬ìŠ¤íŠ¸ë„ ê·¸ëŒ€ë¡œ ë„£ì„ ìˆ˜ ìˆì§€ë§Œ
        # ì—¬ê¸°ì„  ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ invokeë¡œ ì „ë‹¬
        result = llm.invoke(prompt)
        elapsed = time.time() - start_time

        text = (getattr(result, "content", None) or "").strip().lower()
        print(f"âœ… Gemini ì‘ë‹µ ì™„ë£Œ: {elapsed:.2f}s")
        return text or "analyze"

    except Exception as e:
        print(f"âŒ Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return "analyze"


# ====== Ollama (local llama3) ======
def call_ollama(prompt: Union[str, List[HumanMessage]]) -> str:
    """
    ë¡œì»¬ Ollama ì„œë²„ í•„ìš” (ê¸°ë³¸: http://localhost:11434)
    llama3 ëª¨ë¸ ì‚¬ì „ ì„¤ì¹˜: `ollama pull llama3`
    """
    import requests

    model_name = "llama3:latest"
    start = time.time()

    try:
        # ë¬¸ìì—´ë§Œ ì§€ì› (í•„ìš”ì‹œ messages -> string ë³€í™˜ ë¡œì§ ì¶”ê°€)
        if isinstance(prompt, list):
            # ê°„ë‹¨ ë³€í™˜: systemì„ ì•ˆë‚´, userë¥¼ ì´ì–´ë¶™ì—¬ í•˜ë‚˜ì˜ promptë¡œ
            parts = []
            for msg in prompt:
                if isinstance(msg, SystemMessage):
                    parts.append(f"[system]\n{msg.content}")
                elif isinstance(msg, HumanMessage):
                    parts.append(f"[user]\n{msg.content}")
            prompt_text = "\n\n".join(parts)
        else:
            prompt_text = prompt

        resp = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model_name, "prompt": prompt_text, "stream": False},
            timeout=120,
        )
        resp.raise_for_status()
        data = resp.json()
        full_text = (data.get("response", "") or "").strip().lower()
        elapsed = time.time() - start
        print(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {elapsed:.2f}s")
        return full_text or "analyze"

    except Exception as e:
        print(f"âŒ Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return "analyze"


# ====== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì§ì ‘ ì‹¤í–‰ ì‹œ) ======
if __name__ == "__main__":
    demo = "ì†Œë°© ì•ˆì „ ì ê²€ í•­ëª© 3ê°€ì§€ë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì¤˜. í‚¤ëŠ” name, why, howë§Œ ì‚¬ìš©."
    print(call_llm(demo))
